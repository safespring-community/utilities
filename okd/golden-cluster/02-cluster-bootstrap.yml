---
- name: setup okd4 cluster
  hosts: localhost

  vars:
    installer_dir: installer
    okd_cluster_type: "" # Templating fails if this var is non existent. Due to if test in template.
    master_affinity: "soft-anti-affinity"
    network: "default"
    s3_bucket: "ignitionboot"
    s3_filename: "{{okd_cluster_name}}-{{okd_base_domain}}.ign"

  pre_tasks:
    - include_vars: settings.yml

    - name: check for cluster progress
      stat:
        path: "{{ item }}"
      with_items:
        - "{{ installer_dir }}/install-config.yaml"
        - "{{ installer_dir }}/manifests/cluster-scheduler-02-config.yml"
        - "{{ installer_dir }}/bootstrap.ign"
        - "{{ installer_dir }}/master.ign"
        - "{{ installer_dir }}/worker.ign"
      register: _cluster_progress

    - command: "python3 -c \"import openstack; conn = openstack.connect(); print(conn.endpoint_for('s3'))\""
      register: s3_endpoint

    - name: Set fact, s3 endpoint
      ansible.builtin.set_fact:
        s3_endpoint_url: "{{ s3_endpoint.stdout }}"

    - name: Get s3 access key
      ansible.builtin.command:
        "openstack ec2 credential list -c Access  -f value"
      register: s3_access_key_id

    - name: Get s3 secret key
      ansible.builtin.command:
        "openstack ec2 credential list -c Secret  -f value"
      register: s3_secret_key_id

    - name: Ensure s3 credentials set
      ansible.builtin.fail:
        msg: "S3 credentials not set"
      when: 
        - s3_access_key_id.stdout is undefined or s3_access_key_id.stdout == None or s3_access_key_id.stdout | length == 0
        - s3_secret_key_id.stdout is undefined or s3_secret_key_id.stdout == None or s3_secret_key_id.stdout | length == 0

    - name: check for api
      uri:
        url: "https://api.{{ okd_cluster_name }}.{{ okd_base_domain }}:6443/version?timeout=32s"
        validate_certs: false
        return_content: true
      failed_when: false
      register: _cluster_api

    - set_fact:
        install_config_template: install-config.yaml.j2
      when:
        enterprise == False

    - set_fact:
        install_config_template: install-config-ocp.yaml.j2
      when: enterprise == True

    - set_fact:
        openshift_install_cmd: openshift-install
      when: enterprise == False

    - set_fact:
        openshift_install_cmd: "{{ ansible_env.PWD }}/bin/openshift-install"
      when: enterprise ==True


    - set_fact:
        bootstrap_complete: "{{ (_cluster_api.status == 200) | ternary(true, false) }}"

    - name: create directory for installer
      file:
        path: "{{ ansible_env.PWD }}/{{ installer_dir }}"
        state: directory

  tasks:
    - name: create install-config.yaml
      template:
        src: "{{install_config_template}}"
        dest: "{{ ansible_env.PWD }}/{{ installer_dir }}/install-config.yaml"
      register: install_config_yaml
      when:
        - _cluster_progress.results | json_query(cluster_scheduler_config_exists) == false
        - _cluster_progress.results | json_query(ignitions_exist) is not all
      vars:
        ssh_key: "{{ lookup('file', ssh_key_path) }}"
        folder_name: "{{ okd_cluster_name }}"
        cluster_scheduler_config_exists: "[?item==`{{ installer_dir }}/manifests/cluster-scheduler-02-config.yml`].stat.exists | [0]"
        ignitions_exist: "[?item==`{{ installer_dir }}/bootstrap.ign` || item==`{{ installer_dir }}/master.ign` || item==`{{ installer_dir }}/worker.ign`].stat.exists"

    - block:
        - name: openshift-install create manifests
          command: "{{openshift_install_cmd}} --dir={{ installer_dir }}/ create manifests"
          args:
            creates: "{{ ansible_env.PWD }}/{{ installer_dir }}/manifests/cluster-scheduler-02-config.yml"
          register: _openshift_install_create_manifests
      always:
        - debug:
            msg: "{{ _openshift_install_create_manifests[item].split('\n') }}"
          with_items: [stdout, stderr]
          when: item in _openshift_install_create_manifests
      vars:
        install_config_exists: "[?item==`{{ installer_dir }}/install-config.yaml`].stat.exists | [0]"
      when: _cluster_progress.results | json_query(install_config_exists) or
        install_config_yaml is changed

    - name: set mastersSchedulable to false
      replace:
        path: "{{ ansible_env.PWD }}/{{ installer_dir }}/manifests/cluster-scheduler-02-config.yml"
        regexp: "(\\s+)mastersSchedulable: .*"
        replace: "\\1mastersSchedulable: false"
      vars:
        cluster_scheduler_config_exists: "[?item==`{{ installer_dir }}/manifests/cluster-scheduler-02-config.yml`].stat.exists | [0]"
      when: _cluster_progress.results | json_query(cluster_scheduler_config_exists) or
        _openshift_install_create_manifests is changed

    - block:
        - name: openshift-install create ignition-configs
          command: "{{openshift_install_cmd}} --dir={{ installer_dir }} create ignition-configs"
          register: _openshift_install_create_ignition_configs
      always:
        - debug:
            msg: "{{ _openshift_install_create_ignition_configs[item].split('\n') }}"
          with_items: [stdout, stderr]
          when: item in _openshift_install_create_ignition_configs
      when: _cluster_progress.results | json_query(ignitions_exist) is not all
      vars:
        ignitions_exist: "[?item==`{{ installer_dir }}/bootstrap.ign` || item==`{{ installer_dir }}/master.ign` || item==`{{ installer_dir }}/worker.ign`].stat.exists"

    - block:
      - name: Make bucket (idempotent)
        command: "aws --endpoint-url={{ s3_endpoint_url }} s3 mb s3://{{ s3_bucket }}"
        register: _make_bucket
      - name: Put bootstrap ignition file in bucket
        command: aws --endpoint-url={{ s3_endpoint_url }} s3 cp {{ ansible_env.PWD }}/{{ installer_dir }}/bootstrap.ign s3://{{ s3_bucket }}/{{ s3_filename }}
        register: _put_ignition_object
      - name: Presign ignition object
        command: aws --endpoint-url={{ s3_endpoint_url }} s3 presign s3://{{ s3_bucket }}/{{ s3_filename }}
        register: _presign_url
      environment:
        AWS_ACCESS_KEY_ID: "{{ s3_access_key_id.stdout }}"
        AWS_SECRET_ACCESS_KEY: "{{ s3_secret_key_id.stdout }}"

   # Can't use vars because: presedence
    - name: Change facts for boot
      set_fact:
        workersets: ""

    - name: Terraform template openstack image
      template:
        src: "image.tf.j2"
        dest: "image.tf"

    - name: Terraform template boot cluster
      template:
        src: "cluster.tf.j2"
        dest: "cluster.tf"
      vars:
        okd_boot: 1
        ignition_url: "{{ _presign_url.stdout }}"
        network_name: "{{network}}"
        ssh_cidrs: []
        all_ports_cidrs: []
        api_cidrs: []
      when: _presign_url['stdout'] is defined

    - block:
        - name: Terraform init
          command: "terraform init"
          register: _terraform_init
      always:
        - debug:
            msg: "{{ _terraform_init[item].split('\n') }}"
          with_items: [stdout, stderr]
          when: item in _terraform_init

    - block:
        - name: terraform apply with bootstrap
          command: "terraform apply -auto-approve"
          register: _terraform_apply_boot
      always:
        - debug:
            msg: "{{ _terraform_apply_boot[item].split('\n') }}"
          with_items: [stdout, stderr]
          when: item in _terraform_apply_boot
      when: not bootstrap_complete
    - meta: refresh_inventory


- name: Include loadbalancer setup
  import_playbook: lb.yml
  tags:
  - lb

- name: Wait for bootstrap complete
  hosts: localhost
  vars:
    installer_dir: installer
  tasks:
    - block:
          # Bootstrap takes sometimes a long time. Wait for it twice.
        - name: openshift-install wait-for bootstrap-complete
          command: "{{openshift_install_cmd}} --dir={{ installer_dir }}/ wait-for bootstrap-complete"
          ignore_errors: yes
          register: _wait_for_bootstrap

        - name: openshift-install wait-for bootstrap-complete, again
          command: "{{openshift_install_cmd}} --dir={{ installer_dir }}/ wait-for bootstrap-complete"
          ignore_errors: yes
          register: _wait_for_bootstrap

      always:
        - debug:
            msg: "{{ _wait_for_bootstrap[item].split('\n') }}"
          with_items: [stdout, stderr]
          when: item in _wait_for_bootstrap
      when: not bootstrap_complete

    - name: Next steps
      debug:
        msg: "{{ message.split('\n') }}"
      vars:
        message: |
          The cluster control plane is now bootstrapped
          so boot node can be removed and worker nodes
          provisioned. Please run the next playbook
          to do that
